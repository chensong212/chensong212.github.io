---
layout:     post
title:      慕工大SuperMUC-NG服务器相关命令
subtitle:   Linux笔记
date:       2021-05-28
author:     陈陈
header-img:
catalog: true
tags:
    - 编程相关

---

### Login

>ssh -Y skx.supermuc.lrz.de -l ge54cel3

### ssh - Secure Shell on LRZ HPC Systems

[1] https://doku.lrz.de/display/PUBLIC/ssh+-+Secure+Shell+on+LRZ+HPC+Systems

According to the webside, first we do the "Configuration of SSH public-key authentication for access to LRZ systems from outside". 

+ open the "Public keys for **SuperMUC-NG login nodes**", and copy the content;
+ add into the ~/.ssh/known_hosts file (you may need to generate the .ssh directory if it does not exist yet) on your local machine;
+ do according the **Client (your desktop) is a LINUX system**

Generating public/private ecdsa key pair.
>ssh-keygen -t ecdsa -b 521 -a 100

Enter passphrase,e.g. **hereiam**

>ssh-copy-id -i /home/songchen/.ssh/id_ecdsa ge54cel3@skx.supermuc.lrz.de

If successful, the command's response will contain:

>Number of key(s) added: 1


### Check the status of the SuperMUC-NG
[2] https://doku.lrz.de/display/PUBLIC/SuperMUC-NG+Status

### Upload or download files

1. sent DATA FROM LRZ to local computer
>
    scp ge54cel3@skx.supermuc.lrz.de:/dss/dsshome1/0E/ge54cel3/testcase/temp.plt ~/Downloads

2. sent data from local computer to LRZ
>
    scp -r ~/code ge54cel3@skx.supermuc.lrz.de:/dss/dsshome1/0E/ge54cel3/sparta

**scp cannot be invoked on the LRZ system, because all outgoing connections are blocked. You have to use it on your local computer.**

### Compile the Sparta

1. check module
>
      module list
      module show mpi.intel/2019

2. find the libraries
>     echo $MPI_LIB

3. Set the path in the Makefile.foo
>
    MPI_INC = -I/lrz/sys/intel/impi2019u6/impi/2019.6.154/intel64/include /usr/include/c++/4.8 
    MPI_PATH = -L/lrz/sys/intel/impi2019u6/impi/2019.6.154/intel64/lib/  
    MPI_LIB = /lrz/sys/intel/impi2019u6/impi/2019.6.154/intel64/lib/libmpicxx.so.12

4. compile the code
>	make foo

### The sbatch Command
[3] https://doku.lrz.de/display/PUBLIC/Job+Processing+with+SLURM+on+SuperMUC-NG

>
    #!/bin/bash
    # Job Name and Files
    #SBATCH -o ./myjob_output.%j.%N.out
    #SBATCH -D ./
    #SBATCH -J test
    #
    #Notification and type
    #send a notice to email when end: SBATCH --mail-type=END
    #SBATCH --mail-user=insert_your_email_here
    #
    #Setup of execution environment   
    #SBATCH --export=NONE
    #SBATCH --get-user-env
    #SBATCH --account=insert your_projectID_here
    #SBATCH --partition=insert test, micro, general, large or fat
    #
    #SBATCH --nodes=10
    # --- available cores per node 48 ---
    #SBATCH --ntasks-per-node=48
    #
    # Wall clock limit: 
    #SBATCH --time=04:00:00
    #SBATCH --no-requeue
    #
    module load slurm_setup
    #
    #module load gcc/4.8
    #
    #cd $SCRATCH/mydata
    #
    cd /dss/dsshome1/0E/ge54cel3/testcases/unsteady/case1
    mpirun -n $SLURM_NTASKS ./spa_foo < in.config
---


### List of relevant commands
submit a job script
>sbatch	jobname
	
delete or terminate a queued or running job
>scancel job_id

print table of submitted jobs and their state. 
>squeue	-p test

create an interactive SLURM shell
>salloc	
		
execute argument command on the resources assigned to a job. **Note:** must be executed inside an active job (script or interactive environment). **mpiexec** is an alternative and preferred on LRZ system
>srun
	
Display various status information of a running job/step.
>sstat	
	
provide overview of cluster status
>sinfo	

uery and modify SLURM state
>scontrol q

(sacct is not available for users. The detailed usage of these commands can be found in Webpage Ref.[3])
